{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_ml_module.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBhTmaxa4J52bHD/morciX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guruboy001/gurucodes/blob/master/my_ml_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHQ-0aVhYBt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator,TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_B89RhYfa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "class Scaler(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self,columns,with_mean=True,with_std=True,copy=True):\n",
        "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
        "        self.columns = columns\n",
        "        self.mean_ = None\n",
        "        self.var_  = None\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        self.scaler.fit(X[self.columns],y)\n",
        "        self.mean_ = np.mean(X[self.columns])\n",
        "        self.var_ = np.var(X[self.columns])\n",
        "        return self\n",
        "\n",
        "    def transform(self,X,y=None,copy=None):\n",
        "        init_column = X.columns\n",
        "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]),columns=self.columns)\n",
        "        X_unscaled = X.loc[:, ~X.columns.isin(self.columns)]\n",
        "        return pd.concat([X_scaled,X_unscaled],axis=1)[init_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFFWi8ugYp17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AbsenteeismModel():\n",
        "\n",
        "    #Loading the model and the scaler object from pickle\n",
        "    def __init__(self,model_file,scaler_file):\n",
        "        with open('model','rb') as model_file, open('scaler','rb') as scaler_file:\n",
        "            self.reg = pickle.load(model_file)\n",
        "            self.scaler = pickle.load(scaler_file)\n",
        "            self.data = None\n",
        "\n",
        "    \n",
        "    #Take a data file in csv and preprocess it\n",
        "    def load_and_clean_data(self,data_file):\n",
        "        df = pd.read_csv(data_file,delimiter=',')\n",
        "\n",
        "        #save the data for later use\n",
        "        self.df_with_copy = df.copy()\n",
        "\n",
        "        #drop the ID column\n",
        "        df.drop(['ID'],axis=1,inplace=True)\n",
        "\n",
        "        #create a dummy for the absenteeism column\n",
        "        reason_dummy = pd.get_dummies(df['Reason for Absence'],drop_first=True)\n",
        "\n",
        "        #merge the dummies that the reasons are related\n",
        "        first_reason = reason_dummies.loc[:,1:14].max(axis=1)\n",
        "        second_reason = reason_dummies.loc[:,15:17].max(axis=1)\n",
        "        third_reason = reason_dummies.loc[:,18:22].max(axis=1)\n",
        "        fourt_reason = reason_dummies.loc[:,22:].max(axis=1)\n",
        "\n",
        "        #Adding the dummies to the dataframe\n",
        "        df= pd.concat([df,first_reason,second_reason,third_reason,fourt_reason],axis=1)\n",
        "\n",
        "        #removing the reasons column\n",
        "        df.drop(['Reason for Absence'],axis=1,inplace=True)\n",
        "\n",
        "        #dropping the unecessary columns\n",
        "        df.drop(['Reason for Absence','Daily Work Load Average','Distance to Work','days'],axis=1,inplace=True)\n",
        "\n",
        "        #create a checkpoint \n",
        "        self.preprocessed_data = df.copy()\n",
        "\n",
        "        #we need to create a scaler instance at this stage  to be used later \n",
        "        self.data = self.scaler.transform(df)\n",
        "\n",
        "\n",
        "        # A function to calculate the probability of the instance data to be 1 \n",
        "        def predicted_probability(self):\n",
        "            if (self.data is not None):\n",
        "                predict = self.reg.predict_proba(self.data)[:,1]\n",
        "                return predict\n",
        "\n",
        "        # A function that outputs the predicted value for our data\n",
        "\n",
        "        def predicted_output_category(self):\n",
        "            if (self.data is not None):\n",
        "                predicted_output = self.reg.predict(self.data)\n",
        "                return predicted_output\n",
        "        \n",
        "        # A function that returns the predicted values and the probability \n",
        "\n",
        "        def predicted_outputs(self):\n",
        "            if (self.data is not None):\n",
        "                self.preprocessed_data['prediction_probability'] = self.reg.predict_proba(self.data)[:,1]\n",
        "                self.preprocessed_data['predictions'] = self.reg.predictj(self.data)\n",
        "                return self.preprocessed_data\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Pjo3RFbIez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}